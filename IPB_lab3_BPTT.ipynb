{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing RNNs and Implementing Backpropagation Through Time"
      ],
      "metadata": {
        "id": "kAOSpQaHOitp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*This notebook is based on the blog post originally published [here](https://victorzhou.com/blog/intro-to-rnns/#73-gradients). All credits to author of the blog for the original code and explanations.*\n"
      ],
      "metadata": {
        "id": "XY62ZtMT-xhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today's tutorial, we will delve into the world of Recurrent Neural Networks (RNNs), gaining an understanding of their functionality and constructing one from the ground up using only NumPy in Python.\n",
        "\n",
        "Having previously explored backpropagation in feedforward neural networks, we now turn our attention to the complexity introduced by temporal dependencies. Addressing this requires innovative approaches to incorporate the temporal aspect effectively. One such strategy is utilizing \"Backpropagation Through Time\" (BPTT), which we will explore and learn about in our ongoing journey.\n",
        "\n",
        "Here are some resources for your further reading on BPTT in RNNs: [Link1](https://mmuratarat.github.io/2019-02-07/bptt-of-rnn), [Link2](https://towardsdatascience.com/backpropagation-in-rnn-explained-bdf853b4e1c2),\n",
        "[Link3](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html)"
      ],
      "metadata": {
        "id": "Bco-mNFHCpZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset\n",
        "train_data = {\n",
        "  'good': True,\n",
        "  'bad': False,\n",
        "  'happy': True,\n",
        "  'sad': False,\n",
        "  'not good': False,\n",
        "  'not bad': True,\n",
        "  'not happy': False,\n",
        "  'not sad': True,\n",
        "  'very good': True,\n",
        "  'very bad': False,\n",
        "  'very happy': True,\n",
        "  'very sad': False,\n",
        "  'i am happy': True,\n",
        "  'this is good': True,\n",
        "  'i am bad': False,\n",
        "  'this is bad': False,\n",
        "  'i am sad': False,\n",
        "  'this is sad': False,\n",
        "  'i am not happy': False,\n",
        "  'this is not good': False,\n",
        "  'i am not bad': True,\n",
        "  'this is not sad': True,\n",
        "  'i am very happy': True,\n",
        "  'this is very good': True,\n",
        "  'i am very bad': False,\n",
        "  'this is very sad': False,\n",
        "  'this is very happy': True,\n",
        "  'i am good not bad': True,\n",
        "  'this is good not bad': True,\n",
        "  'i am bad not good': False,\n",
        "  'i am good and happy': True,\n",
        "  'this is not good and not happy': False,\n",
        "  'i am not at all good': False,\n",
        "  'i am not at all bad': True,\n",
        "  'i am not at all happy': False,\n",
        "  'this is not at all sad': True,\n",
        "  'this is not at all happy': False,\n",
        "  'i am good right now': True,\n",
        "  'i am bad right now': False,\n",
        "  'this is bad right now': False,\n",
        "  'i am sad right now': False,\n",
        "  'i was good earlier': True,\n",
        "  'i was happy earlier': True,\n",
        "  'i was bad earlier': False,\n",
        "  'i was sad earlier': False,\n",
        "  'i am very bad right now': False,\n",
        "  'this is very good right now': True,\n",
        "  'this is very sad right now': False,\n",
        "  'this was bad earlier': False,\n",
        "  'this was very good earlier': True,\n",
        "  'this was very bad earlier': False,\n",
        "  'this was very happy earlier': True,\n",
        "  'this was very sad earlier': False,\n",
        "  'i was good and not bad earlier': True,\n",
        "  'i was not good and not happy earlier': False,\n",
        "  'i am not at all bad or sad right now': True,\n",
        "  'i am not at all good or happy right now': False,\n",
        "  'this was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "  'this is happy': True,\n",
        "  'i am good': True,\n",
        "  'this is not happy': False,\n",
        "  'i am not good': False,\n",
        "  'this is not bad': True,\n",
        "  'i am not sad': True,\n",
        "  'i am very good': True,\n",
        "  'this is very bad': False,\n",
        "  'i am very sad': False,\n",
        "  'this is bad not good': False,\n",
        "  'this is good and happy': True,\n",
        "  'i am not good and not happy': False,\n",
        "  'i am not at all sad': True,\n",
        "  'this is not at all good': False,\n",
        "  'this is not at all bad': True,\n",
        "  'this is good right now': True,\n",
        "  'this is sad right now': False,\n",
        "  'this is very bad right now': False,\n",
        "  'this was good earlier': True,\n",
        "  'i was not happy and not good earlier': False,\n",
        "}"
      ],
      "metadata": {
        "id": "eDMPlYtCHsqe",
        "cellView": "form"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Pre-Processing\n"
      ],
      "metadata": {
        "id": "H21qmB-RDS5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll have to do some pre-processing to get the data into a usable format. To start, we’ll construct a vocabulary of all words that exist in our data:\n",
        "\n"
      ],
      "metadata": {
        "id": "RYtDJZBbDZ0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vocabulary.\n",
        "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
        "vocab_size = len(vocab)\n",
        "print('%d unique words found' % vocab_size) # 18 unique words found"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QifDwFKiDVYj",
        "outputId": "a88861d2-43a3-4ef3-95c1-c24249a8b084"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 unique words found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab now holds a list of all words that appear in at least one training text. Next, we’ll assign an integer index to represent each word in our vocab."
      ],
      "metadata": {
        "id": "1R64tZw3DqGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign indices to each word.\n",
        "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
        "idx_to_word = { i: w for i, w in enumerate(vocab) }\n",
        "print(word_to_idx['good']) # 16 (this may change)\n",
        "print(idx_to_word[0]) # sad (this may change)"
      ],
      "metadata": {
        "id": "4pUR-FERDqjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef2622a-d5a8-4c9e-aac9-36145a2b77a5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "am\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now represent any given word with its corresponding integer index! This is necessary because RNNs can’t understand words - we have to give them numbers.\n",
        "\n",
        "Finally, recall that each input $x_i$\n",
        "  to our RNN is a vector. We’ll use one-hot vectors, which contain all zeros except for a single one. The “one” in each one-hot vector will be at the word’s corresponding integer index.\n",
        "\n",
        "Since we have 18 unique words in our vocabulary, each $x_i$ will be a 18-dimensional one-hot vector."
      ],
      "metadata": {
        "id": "D-ydK84tD19k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def createInputs(text):\n",
        "  '''\n",
        "  Returns an array of one-hot vectors representing the words\n",
        "  in the input text string.\n",
        "  - text is a string\n",
        "  - Each one-hot vector has shape (vocab_size, 1)\n",
        "  '''\n",
        "  inputs = []\n",
        "  for w in text.split(' '):\n",
        "    v = np.zeros((vocab_size, 1))\n",
        "    v[word_to_idx[w]] = 1\n",
        "    inputs.append(v)\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "BWEMllpyEDD1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use createInputs() later to create vector inputs to pass in to our RNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "c0EBQDxDEGdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Forward Phase\n"
      ],
      "metadata": {
        "id": "2WFy5zFAEI7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It’s time to start implementing our RNN! We’ll start by initializing the 3 weights and 2 biases our RNN needs:"
      ],
      "metadata": {
        "id": "_EcsuIILEOJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "class RNN:\n",
        "  # A Vanilla Recurrent Neural Network.\n",
        "\n",
        " def __init__(self, input_size, output_size, hidden_size=64):\n",
        "    # Weights\n",
        "    self.Whh = randn(hidden_size, hidden_size) / 1000\n",
        "    self.Wxh = randn(hidden_size, input_size) / 1000\n",
        "    self.Why = randn(output_size, hidden_size) / 1000\n",
        "\n",
        "    # Biases\n",
        "    self.bh = np.zeros((hidden_size, 1))\n",
        "    self.by = np.zeros((output_size, 1))\n",
        "\n",
        " def forward(self, inputs):\n",
        "   '''\n",
        "   Perform a forward pass of the RNN using the given inputs.\n",
        "    Returns the final output and hidden state.\n",
        "    - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
        "    '''\n",
        "   h = np.zeros((self.Whh.shape[0], 1))\n",
        "   self.last_inputs = inputs\n",
        "   self.last_hs = { 0: h }\n",
        "    # Perform each step of the RNN\n",
        "   for i, x in enumerate(inputs):\n",
        "    h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "    self.last_hs[i + 1] = h\n",
        "\n",
        "    # Compute the output\n",
        "    y = self.Why @ h + self.by\n",
        "    return y, h"
      ],
      "metadata": {
        "id": "EMi6M9gFELUH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use np.random.randn() to initialize our weights from the standard normal distribution.\n",
        "\n",
        "Next, let’s implement our RNN’s forward pass."
      ],
      "metadata": {
        "id": "ElBxq4emEZFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty simple, right? Note that we initialized\n",
        "ℎ\n",
        "h to the zero vector for the first step, since there’s no previous\n",
        "ℎ\n",
        "h we can use at that point.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlWaDnziEiCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Backward Phase\n"
      ],
      "metadata": {
        "id": "q5WJoha4EqVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to train our RNN, we first need a loss function. We’ll use cross-entropy loss, which is often paired with Softmax. Here’s how we calculate it:\n",
        "\n",
        "$L=−ln(p\n",
        "c\n",
        "​\n",
        " )$\n",
        "\n",
        " where\n",
        "$\n",
        "p_c\n",
        "$  is our RNN’s predicted probability for the correct class (positive or negative). For example, if a positive text is predicted to be 90% positive by our RNN, the loss is:\n",
        "\n",
        "$L=−ln(0.90)$ = 0.105\n",
        "\n",
        "Now that we have a loss, we’ll train our RNN using gradient descent to minimize loss. That means it’s time to derive some gradients!\n",
        "\n"
      ],
      "metadata": {
        "id": "tKiAQ9iCEwRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to edit our forward phase to cache some data for use in the backward phase. While we’re at it, we’ll also setup the skeleton for our backwards phase. Here’s what that looks like:"
      ],
      "metadata": {
        "id": "VvSgwMSqFql5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradients"
      ],
      "metadata": {
        "id": "IheYINNJGhsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#def backprop_(self, d_y, learn_rate=2e-2):\n",
        "\n",
        "    #Perform a backward pass of the RNN.\n",
        "   # - d_y (dL/dy) has shape (output_size, 1).\n",
        "    #- learn_rate is a float.\n",
        "\n",
        "    #n = len(self.last_inputs)\n",
        "\n",
        "    # Calculate dL/dWhy and dL/dby.\n",
        "    #d_Why = d_y @ self.last_hs[n].T\n",
        "    #d_by = d_y\n",
        "\n",
        "    # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
        "    #d_Whh = np.zeros(self.Whh.shape)\n",
        "    #d_Wxh = np.zeros(self.Wxh.shape)\n",
        "    #d_bh = np.zeros(self.bh.shape)\n",
        "\n",
        "    # Calculate dL/dh for the last h.\n",
        "    #d_h = self.Why.T @ d_y\n",
        "\n",
        "    # Backpropagate through time.\n",
        "    #for t in reversed(range(n)):\n",
        "      # An intermediate value: dL/dh * (1 - h^2)\n",
        "      #temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
        "\n",
        "      # dL/db = dL/dh * (1 - h^2)\n",
        "      #d_bh += temp\n",
        "\n",
        "      # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
        "      #d_Whh += temp @ self.last_hs[t].T\n",
        "\n",
        "      # dL/dWxh = dL/dh * (1 - h^2) * x\n",
        "      #d_Wxh += temp @ self.last_inputs[t].T\n",
        "\n",
        "      # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
        "      #d_h = self.Whh @ temp\n",
        "\n",
        "    # Clip to prevent exploding gradients.\n",
        "    #for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
        "      #np.clip(d, -1, 1, out=d)\n",
        "\n",
        "    # Update weights and biases using gradient descent.\n",
        "    #self.Whh -= learn_rate * d_Whh\n",
        "    #self.Wxh -= learn_rate * d_Wxh\n",
        "    #self.Why -= learn_rate * d_Why\n",
        "    #self.bh -= learn_rate * d_bh\n",
        "    #self.by -= learn_rate * d_by"
      ],
      "metadata": {
        "id": "AM3JLImO6uFv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting it altogether!"
      ],
      "metadata": {
        "id": "6xaubC_N6w73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import randn\n",
        "class RNN:\n",
        "  # A Vanilla Recurrent Neural Network.\n",
        "\n",
        "  def __init__(self, input_size, output_size, hidden_size=64):\n",
        "    # Weights\n",
        "    self.Whh = randn(hidden_size, hidden_size) / 1000\n",
        "    self.Wxh = randn(hidden_size, input_size) / 1000\n",
        "    self.Why = randn(output_size, hidden_size) / 1000\n",
        "\n",
        "    # Biases\n",
        "    self.bh = np.zeros((hidden_size, 1))\n",
        "    self.by = np.zeros((output_size, 1))\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    '''\n",
        "    Perform a forward pass of the RNN using the given inputs.\n",
        "    Returns the final output and hidden state.\n",
        "    - inputs is an array of one-hot vectors with shape (input_size, 1).\n",
        "    '''\n",
        "    h = np.zeros((self.Whh.shape[0], 1))\n",
        "\n",
        "    self.last_inputs = inputs\n",
        "    self.last_hs = { 0: h }\n",
        "\n",
        "    # Perform each step of the RNN\n",
        "    for i, x in enumerate(inputs):\n",
        "      h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "      self.last_hs[i + 1] = h\n",
        "\n",
        "    # Compute the output\n",
        "    y = self.Why @ h + self.by\n",
        "\n",
        "    return y, h\n",
        "\n",
        "  def backprop(self, d_y, learn_rate=2e-2):\n",
        "    '''\n",
        "    Perform a backward pass of the RNN.\n",
        "    - d_y (dL/dy) has shape (output_size, 1).\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    n = len(self.last_inputs)\n",
        "\n",
        "    # Calculate dL/dWhy and dL/dby.\n",
        "    d_Why = d_y @ self.last_hs[n].T\n",
        "    d_by = d_y\n",
        "\n",
        "    # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
        "    d_Whh = np.zeros(self.Whh.shape)\n",
        "    d_Wxh = np.zeros(self.Wxh.shape)\n",
        "    d_bh = np.zeros(self.bh.shape)\n",
        "\n",
        "    # Calculate dL/dh for the last h.\n",
        "    d_h = self.Why.T @ d_y\n",
        "\n",
        "    # Backpropagate through time.\n",
        "    for t in reversed(range(n)):\n",
        "      # An intermediate value: dL/dh * (1 - h^2)\n",
        "      temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
        "\n",
        "      # dL/db = dL/dh * (1 - h^2)\n",
        "      d_bh += temp\n",
        "\n",
        "      # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
        "      d_Whh += temp @ self.last_hs[t].T\n",
        "\n",
        "      # dL/dWxh = dL/dh * (1 - h^2) * x\n",
        "      d_Wxh += temp @ self.last_inputs[t].T\n",
        "\n",
        "      # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
        "      d_h = self.Whh @ temp\n",
        "\n",
        "    # Clip to prevent exploding gradients.\n",
        "    for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
        "      np.clip(d, -1, 1, out=d)\n",
        "\n",
        "    # Update weights and biases using gradient descent.\n",
        "    self.Whh -= learn_rate * d_Whh\n",
        "    self.Wxh -= learn_rate * d_Wxh\n",
        "    self.Why -= learn_rate * d_Why\n",
        "    self.bh -= learn_rate * d_bh\n",
        "    self.by -= learn_rate * d_by"
      ],
      "metadata": {
        "id": "t13EenMfGx-D"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Simple Forward Pass\n",
        "Before the training"
      ],
      "metadata": {
        "id": "MYAfyI8A6WJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(xs):\n",
        "  # Applies the Softmax Function to the input array.\n",
        "   return np.exp(xs) / sum(np.exp(xs))\n",
        "\n",
        "#Initialize our RNN!\n",
        "rnn =  RNN(vocab_size,2)\n",
        "\n",
        "\n",
        "inputs = createInputs('i am very good')\n",
        "out, h = rnn.forward(inputs)\n",
        "probs = softmax(out)\n",
        "print(probs) # [[0.50000095], [0.49999905]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H66RbbGWEmId",
        "outputId": "89f8ac7e-a934-4278-e0fe-e02cccc911e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.49999478]\n",
            " [0.50000522]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Our RNN"
      ],
      "metadata": {
        "id": "64uBnrkF6Bg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over each training example\n",
        "rnn = RNN(vocab_size, 2)\n",
        "for x, y in train_data.items():\n",
        "  inputs = createInputs(x)\n",
        "  target = int(y)\n",
        "\n",
        "  # Forward\n",
        "  out, _ = rnn.forward(inputs)\n",
        "  probs = softmax(out)\n",
        "\n",
        "  # Build dL/dy\n",
        "  d_L_d_y = probs\n",
        "  d_L_d_y[target] -= 1\n",
        "\n",
        "  # Backward\n",
        " #rnn.backprop(d_L_d_y)"
      ],
      "metadata": {
        "id": "k7MmmfsUGGfF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing our RNN"
      ],
      "metadata": {
        "id": "HpcyxZLIG2JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def processData(data, backprop=True):\n",
        "  '''\n",
        "  Returns the RNN's loss and accuracy for the given data.\n",
        "  - data is a dictionary mapping text to True or False.\n",
        "  - backprop determines if the backward phase should be run.\n",
        "  '''\n",
        "  items = list(data.items())\n",
        "  random.shuffle(items)\n",
        "\n",
        "  loss = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for x, y in items:\n",
        "    inputs = createInputs(x)\n",
        "    target = int(y)\n",
        "\n",
        "    # Forward\n",
        "    out, _ = rnn.forward(inputs)\n",
        "    probs = softmax(out)\n",
        "\n",
        "    # Calculate loss / accuracy\n",
        "    loss -= np.log(probs[target])\n",
        "    num_correct += int(np.argmax(probs) == target)\n",
        "\n",
        "    if backprop:\n",
        "      # Build dL/dy\n",
        "      d_L_d_y = probs\n",
        "      d_L_d_y[target] -= 1\n",
        "\n",
        "      # Backward\n",
        "      rnn.backprop(d_L_d_y)\n",
        "\n",
        "  return loss / len(data), num_correct / len(data)\n"
      ],
      "metadata": {
        "id": "0C6qiBLgG8rP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can write the training loop:\n",
        "\n"
      ],
      "metadata": {
        "id": "zftV_7INRKr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "  train_loss, train_acc = processData(train_data)\n",
        "\n",
        "  if epoch % 100 == 99:\n",
        "    print('--- Epoch %d' % (epoch + 1))\n",
        "    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
        "\n",
        "    test_loss, test_acc = processData(test_data, backprop=False)\n",
        "    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dAi4aU2jC8s",
        "outputId": "a0419e48-2dba-438e-c050-c048311f7e9b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 100\n",
            "Train:\tLoss 0.688 | Accuracy: 0.569\n",
            "Test:\tLoss 0.698 | Accuracy: 0.500\n",
            "--- Epoch 200\n",
            "Train:\tLoss 0.668 | Accuracy: 0.603\n",
            "Test:\tLoss 0.718 | Accuracy: 0.550\n",
            "--- Epoch 300\n",
            "Train:\tLoss 0.555 | Accuracy: 0.724\n",
            "Test:\tLoss 0.573 | Accuracy: 0.700\n",
            "--- Epoch 400\n",
            "Train:\tLoss 0.366 | Accuracy: 0.862\n",
            "Test:\tLoss 0.438 | Accuracy: 0.800\n",
            "--- Epoch 500\n",
            "Train:\tLoss 0.008 | Accuracy: 1.000\n",
            "Test:\tLoss 0.010 | Accuracy: 1.000\n",
            "--- Epoch 600\n",
            "Train:\tLoss 0.002 | Accuracy: 1.000\n",
            "Test:\tLoss 0.003 | Accuracy: 1.000\n",
            "--- Epoch 700\n",
            "Train:\tLoss 0.001 | Accuracy: 1.000\n",
            "Test:\tLoss 0.002 | Accuracy: 1.000\n",
            "--- Epoch 800\n",
            "Train:\tLoss 0.001 | Accuracy: 1.000\n",
            "Test:\tLoss 0.001 | Accuracy: 1.000\n",
            "--- Epoch 900\n",
            "Train:\tLoss 0.001 | Accuracy: 1.000\n",
            "Test:\tLoss 0.001 | Accuracy: 1.000\n",
            "--- Epoch 1000\n",
            "Train:\tLoss 0.001 | Accuracy: 1.000\n",
            "Test:\tLoss 0.001 | Accuracy: 1.000\n"
          ]
        }
      ]
    }
  ]
}